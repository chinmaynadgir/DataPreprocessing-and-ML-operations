{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 2: Data Preprocessing and Cleaning\n",
        "\n",
        "**Author:** Chinmay Nadgir  \n",
        "**Date:** October 2025  \n",
        "**Purpose:** Demonstrate professional data preprocessing techniques for production-ready analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#intro)\n",
        "2. [Setup & Data Loading](#setup)\n",
        "3. [Data Quality Assessment](#assessment)\n",
        "4. [Handling Missing Values](#missing)\n",
        "5. [Data Type Conversions](#datatypes)\n",
        "6. [Outlier Detection and Treatment](#outliers)\n",
        "7. [Feature Engineering](#features)\n",
        "8. [Data Normalization and Scaling](#scaling)\n",
        "9. [Encoding Categorical Variables](#encoding)\n",
        "10. [Final Validation](#validation)\n",
        "11. [Summary](#summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='intro'></a>\n",
        "## 1. Introduction\n",
        "\n",
        "Data preprocessing is the critical foundation of any data science project. Raw data typically contains inconsistencies, missing values, and outliers that must be addressed before analysis.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Assess data quality systematically\n",
        "- Handle missing data with appropriate strategies\n",
        "- Detect and treat outliers\n",
        "- Engineer new features from existing data\n",
        "- Normalize and scale numeric variables\n",
        "- Encode categorical variables properly\n",
        "\n",
        "**Key Principle:** Document all preprocessing decisions for reproducibility and transparency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='setup'></a>\n",
        "## 2. Setup & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 2)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Display versions\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"matplotlib: {plt.matplotlib.__version__}\")\n",
        "print(f\"seaborn: {sns.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the sample dataset from Module 1\n",
        "data_dir = Path('data')\n",
        "\n",
        "# Create sample dataset if Module 1 wasn't run\n",
        "if not (data_dir / 'sample_data.csv').exists():\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    np.random.seed(42)\n",
        "    df = pd.DataFrame({\n",
        "        'customer_id': range(1, 101),\n",
        "        'name': [f'Customer_{i}' for i in range(1, 101)],\n",
        "        'age': np.random.randint(18, 70, 100),\n",
        "        'purchase_amount': np.random.uniform(10, 1000, 100).round(2),\n",
        "        'category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], 100),\n",
        "        'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
        "        'loyalty_member': np.random.choice([True, False], 100)\n",
        "    })\n",
        "    # Introduce missing values\n",
        "    df.loc[5:8, 'age'] = np.nan\n",
        "    df.loc[15:17, 'purchase_amount'] = np.nan\n",
        "    df.to_csv(data_dir / 'sample_data.csv', index=False)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(data_dir / 'sample_data.csv', parse_dates=['date'])\n",
        "df_original = df.copy()  # Preserve original for comparison\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='assessment'></a>\n",
        "## 3. Data Quality Assessment\n",
        "\n",
        "Begin with a comprehensive assessment of data quality issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assess_data_quality(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate a comprehensive data quality report.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame to assess\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Quality report with metrics per column\n",
        "    \"\"\"\n",
        "    quality_report = pd.DataFrame({\n",
        "        'Data_Type': df.dtypes,\n",
        "        'Non_Null_Count': df.count(),\n",
        "        'Null_Count': df.isnull().sum(),\n",
        "        'Null_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
        "        'Unique_Values': df.nunique(),\n",
        "        'Duplicate_Rows': [df.duplicated().sum()] * len(df.columns)\n",
        "    })\n",
        "    \n",
        "    return quality_report\n",
        "\n",
        "quality_report = assess_data_quality(df)\n",
        "print(\"\\nData Quality Report:\")\n",
        "print(\"=\" * 80)\n",
        "display(quality_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(f\"Removing {duplicates} duplicate rows...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"New shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize missing data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "missing_data = df.isnull().sum()\n",
        "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    missing_data.plot(kind='bar', ax=ax, color='coral')\n",
        "    ax.set_title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Column', fontsize=12)\n",
        "    ax.set_ylabel('Count of Missing Values', fontsize=12)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No missing values to visualize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='missing'></a>\n",
        "## 4. Handling Missing Values\n",
        "\n",
        "Missing data strategies depend on the amount and pattern of missingness.\n",
        "\n",
        "**Strategies:**\n",
        "- Drop columns with >50% missing\n",
        "- Impute numeric: mean, median, or forward-fill\n",
        "- Impute categorical: mode or 'Unknown'\n",
        "- Document reasoning for each decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify columns with >50% missing values\n",
        "missing_threshold = 0.5\n",
        "missing_pct = df.isnull().sum() / len(df)\n",
        "cols_to_drop = missing_pct[missing_pct > missing_threshold].index.tolist()\n",
        "\n",
        "if cols_to_drop:\n",
        "    print(f\"Dropping columns with >{missing_threshold*100}% missing: {cols_to_drop}\")\n",
        "    df = df.drop(columns=cols_to_drop)\n",
        "else:\n",
        "    print(f\"No columns exceed {missing_threshold*100}% missing threshold.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impute numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        missing_count = df[col].isnull().sum()\n",
        "        # Use median for robustness to outliers\n",
        "        median_value = df[col].median()\n",
        "        df[col].fillna(median_value, inplace=True)\n",
        "        print(f\"Imputed {missing_count} missing values in '{col}' with median: {median_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impute categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        missing_count = df[col].isnull().sum()\n",
        "        # Use mode (most frequent value)\n",
        "        mode_value = df[col].mode()[0]\n",
        "        df[col].fillna(mode_value, inplace=True)\n",
        "        print(f\"Imputed {missing_count} missing values in '{col}' with mode: {mode_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify no missing values remain\n",
        "remaining_missing = df.isnull().sum().sum()\n",
        "print(f\"\\nTotal missing values after imputation: {remaining_missing}\")\n",
        "\n",
        "if remaining_missing == 0:\n",
        "    print(\"All missing values have been handled successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='datatypes'></a>\n",
        "## 5. Data Type Conversions\n",
        "\n",
        "Correct data types improve memory efficiency and enable proper operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data types before conversion:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert categorical columns with low cardinality to category dtype\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() / len(df) < 0.5:  # Less than 50% unique values\n",
        "        df[col] = df[col].astype('category')\n",
        "        print(f\"Converted '{col}' to category dtype\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert boolean columns\n",
        "bool_cols = ['loyalty_member']\n",
        "for col in bool_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype('bool')\n",
        "        print(f\"Converted '{col}' to bool dtype\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nData types after conversion:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "print(f\"Memory reduction achieved through type optimization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='outliers'></a>\n",
        "## 6. Outlier Detection and Treatment\n",
        "\n",
        "Outliers can skew analysis. We use IQR method and Z-score for detection.\n",
        "\n",
        "**IQR Method:** Values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_iqr(df: pd.DataFrame, column: str) -> Tuple[pd.Series, float, float]:\n",
        "    \"\"\"\n",
        "    Detect outliers using IQR method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame\n",
        "    column : str\n",
        "        Column name\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple[pd.Series, float, float]\n",
        "        Boolean series of outliers, lower bound, upper bound\n",
        "    \"\"\"\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# Detect outliers in numeric columns\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if col != 'customer_id':  # Skip ID columns\n",
        "        outliers, lower, upper = detect_outliers_iqr(df, col)\n",
        "        outlier_count = outliers.sum()\n",
        "        outlier_summary[col] = {\n",
        "            'count': outlier_count,\n",
        "            'percentage': (outlier_count / len(df) * 100).round(2),\n",
        "            'lower_bound': lower,\n",
        "            'upper_bound': upper\n",
        "        }\n",
        "\n",
        "print(\"\\nOutlier Detection Summary:\")\n",
        "print(\"=\" * 80)\n",
        "for col, info in outlier_summary.items():\n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  Count: {info['count']} ({info['percentage']}%)\")\n",
        "    print(f\"  Bounds: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize outliers with boxplots\n",
        "numeric_cols_to_plot = [col for col in numeric_cols if col != 'customer_id']\n",
        "\n",
        "if numeric_cols_to_plot:\n",
        "    fig, axes = plt.subplots(1, len(numeric_cols_to_plot), figsize=(14, 5))\n",
        "    if len(numeric_cols_to_plot) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols_to_plot):\n",
        "        axes[idx].boxplot(df[col].dropna(), vert=True)\n",
        "        axes[idx].set_title(f'{col}', fontweight='bold')\n",
        "        axes[idx].set_ylabel('Value')\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Outlier Detection - Boxplots', fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cap outliers using IQR bounds\n",
        "for col in numeric_cols:\n",
        "    if col != 'customer_id':\n",
        "        outliers, lower, upper = detect_outliers_iqr(df, col)\n",
        "        if outliers.sum() > 0:\n",
        "            original_outliers = outliers.sum()\n",
        "            df[col] = df[col].clip(lower=lower, upper=upper)\n",
        "            print(f\"Capped {original_outliers} outliers in '{col}' to [{lower:.2f}, {upper:.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='features'></a>\n",
        "## 7. Feature Engineering\n",
        "\n",
        "Create new features from existing data to enhance analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract datetime features if date column exists\n",
        "if 'date' in df.columns:\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek\n",
        "    df['day_name'] = df['date'].dt.day_name()\n",
        "    df['quarter'] = df['date'].dt.quarter\n",
        "    print(\"Extracted datetime features: year, month, day_of_week, day_name, quarter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create binned features for continuous variables\n",
        "if 'age' in df.columns:\n",
        "    df['age_group'] = pd.cut(\n",
        "        df['age'],\n",
        "        bins=[0, 25, 35, 50, 100],\n",
        "        labels=['18-25', '26-35', '36-50', '51+']\n",
        "    )\n",
        "    print(\"Created 'age_group' feature\")\n",
        "\n",
        "if 'purchase_amount' in df.columns:\n",
        "    df['purchase_category'] = pd.cut(\n",
        "        df['purchase_amount'],\n",
        "        bins=[0, 100, 300, 500, 1000],\n",
        "        labels=['Low', 'Medium', 'High', 'Very High']\n",
        "    )\n",
        "    print(\"Created 'purchase_category' feature\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nDataset shape after feature engineering: {df.shape}\")\n",
        "print(f\"New columns: {[col for col in df.columns if col not in df_original.columns]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='scaling'></a>\n",
        "## 8. Data Normalization and Scaling\n",
        "\n",
        "Scaling brings features to similar ranges, important for distance-based algorithms.\n",
        "\n",
        "**Methods:**\n",
        "- StandardScaler: Mean=0, Std=1 (assumes normal distribution)\n",
        "- MinMaxScaler: Scale to [0,1] range\n",
        "- RobustScaler: Uses median and IQR (robust to outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numeric columns for scaling (exclude IDs and engineered categorical features)\n",
        "cols_to_scale = ['age', 'purchase_amount']\n",
        "cols_to_scale = [col for col in cols_to_scale if col in df.columns]\n",
        "\n",
        "if cols_to_scale:\n",
        "    # Create copies for comparison\n",
        "    df_standard = df.copy()\n",
        "    df_minmax = df.copy()\n",
        "    df_robust = df.copy()\n",
        "    \n",
        "    # StandardScaler\n",
        "    scaler_standard = StandardScaler()\n",
        "    df_standard[[f'{col}_standard' for col in cols_to_scale]] = scaler_standard.fit_transform(df[cols_to_scale])\n",
        "    \n",
        "    # MinMaxScaler\n",
        "    scaler_minmax = MinMaxScaler()\n",
        "    df_minmax[[f'{col}_minmax' for col in cols_to_scale]] = scaler_minmax.fit_transform(df[cols_to_scale])\n",
        "    \n",
        "    # RobustScaler\n",
        "    scaler_robust = RobustScaler()\n",
        "    df_robust[[f'{col}_robust' for col in cols_to_scale]] = scaler_robust.fit_transform(df[cols_to_scale])\n",
        "    \n",
        "    print(\"Scaling applied to:\", cols_to_scale)\n",
        "    print(\"\\nScaled columns created with suffixes: _standard, _minmax, _robust\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare scaling methods\n",
        "if cols_to_scale:\n",
        "    comparison_col = cols_to_scale[0]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    # Original\n",
        "    axes[0, 0].hist(df[comparison_col], bins=20, color='steelblue', edgecolor='black')\n",
        "    axes[0, 0].set_title(f'Original: {comparison_col}', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Value')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # StandardScaler\n",
        "    axes[0, 1].hist(df_standard[f'{comparison_col}_standard'], bins=20, color='coral', edgecolor='black')\n",
        "    axes[0, 1].set_title('StandardScaler (Z-score)', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Value')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    # MinMaxScaler\n",
        "    axes[1, 0].hist(df_minmax[f'{comparison_col}_minmax'], bins=20, color='lightgreen', edgecolor='black')\n",
        "    axes[1, 0].set_title('MinMaxScaler [0,1]', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Value')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # RobustScaler\n",
        "    axes[1, 1].hist(df_robust[f'{comparison_col}_robust'], bins=20, color='plum', edgecolor='black')\n",
        "    axes[1, 1].set_title('RobustScaler (Median/IQR)', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Value')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.suptitle('Comparison of Scaling Methods', fontsize=14, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Add scaled columns to main dataframe (using StandardScaler as default)\n",
        "    for col in cols_to_scale:\n",
        "        df[f'{col}_scaled'] = df_standard[f'{col}_standard']\n",
        "    print(f\"\\nAdded scaled versions of {cols_to_scale} to main dataframe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='encoding'></a>\n",
        "## 9. Encoding Categorical Variables\n",
        "\n",
        "Convert categorical variables to numeric for analysis.\n",
        "\n",
        "**Methods:**\n",
        "- One-Hot Encoding: For nominal variables (no order)\n",
        "- Label Encoding: For ordinal variables (with order)\n",
        "- Frequency Encoding: Useful for high-cardinality features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding for nominal categorical variables\n",
        "nominal_cols = ['category']\n",
        "nominal_cols = [col for col in nominal_cols if col in df.columns]\n",
        "\n",
        "if nominal_cols:\n",
        "    df_encoded = pd.get_dummies(df, columns=nominal_cols, prefix=nominal_cols, drop_first=False)\n",
        "    print(f\"Applied one-hot encoding to: {nominal_cols}\")\n",
        "    print(f\"Shape after encoding: {df_encoded.shape}\")\n",
        "    print(f\"\\nNew encoded columns:\")\n",
        "    new_cols = [col for col in df_encoded.columns if col not in df.columns]\n",
        "    for col in new_cols:\n",
        "        print(f\"  - {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label encoding for ordinal variables\n",
        "ordinal_mapping = {\n",
        "    'Low': 1,\n",
        "    'Medium': 2,\n",
        "    'High': 3,\n",
        "    'Very High': 4\n",
        "}\n",
        "\n",
        "if 'purchase_category' in df.columns:\n",
        "    df['purchase_category_encoded'] = df['purchase_category'].map(ordinal_mapping)\n",
        "    print(\"\\nApplied label encoding to 'purchase_category'\")\n",
        "    print(f\"Mapping: {ordinal_mapping}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='validation'></a>\n",
        "## 10. Final Validation\n",
        "\n",
        "Verify data is clean and ready for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Final Data Validation:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics (Numeric Columns):\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned data\n",
        "output_path = data_dir / 'cleaned_data.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"\\nCleaned data saved to: {output_path}\")\n",
        "\n",
        "# Save encoded version if it exists\n",
        "if 'df_encoded' in locals():\n",
        "    encoded_path = data_dir / 'encoded_data.csv'\n",
        "    df_encoded.to_csv(encoded_path, index=False)\n",
        "    print(f\"Encoded data saved to: {encoded_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='summary'></a>\n",
        "## 11. Summary\n",
        "\n",
        "### Preprocessing Steps Completed\n",
        "\n",
        "1. **Data Quality Assessment:** Identified missing values, duplicates, and data type issues\n",
        "2. **Missing Value Handling:** Imputed numeric columns with median, categorical with mode\n",
        "3. **Data Type Optimization:** Converted to appropriate types for memory efficiency\n",
        "4. **Outlier Treatment:** Detected using IQR method and capped extreme values\n",
        "5. **Feature Engineering:** Created datetime features and binned continuous variables\n",
        "6. **Normalization:** Applied StandardScaler, MinMaxScaler, and RobustScaler\n",
        "7. **Encoding:** One-hot encoded nominal variables, label encoded ordinal variables\n",
        "8. **Validation:** Verified clean data with no missing values or duplicates\n",
        "\n",
        "### Key Decisions Documented\n",
        "\n",
        "- Used median imputation for numeric features (robust to outliers)\n",
        "- Applied IQR method with 1.5 multiplier for outlier detection\n",
        "- Converted low-cardinality string columns to category dtype\n",
        "- Created age groups: 18-25, 26-35, 36-50, 51+\n",
        "- Created purchase categories: Low, Medium, High, Very High\n",
        "- Used StandardScaler for final scaled versions\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Data is now ready for:\n",
        "- Statistical analysis (Module 3)\n",
        "- Visualization (Module 4)\n",
        "- Exploratory Data Analysis (Module 5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}